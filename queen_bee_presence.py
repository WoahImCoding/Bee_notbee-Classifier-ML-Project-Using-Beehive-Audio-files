# -*- coding: utf-8 -*-
"""Queen_Bee_Presence.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gse9WYYzyq26DZb1_7W6gj1cSV43awXI

requirements.txt
"""

!pip install kagglehub librosa scikit-learn tensorflow muda jams --quiet

!pip install soundfile --quiet

!pip install numpy==2.0.0 --quiet

!pip install --upgrade librosa numba

import numpy as np

np.float_ = np.float64

import glob
import os
import librosa
import pdb
import csv
import json
import re
import numpy as np
import random
import librosa.display
import IPython.display as ipd
from sklearn import preprocessing, svm
from collections import Counter
from matplotlib import pyplot as plt
import muda
import jams
import soundfile as sf
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, average_precision_score,
    roc_curve, precision_recall_curve, accuracy_score, f1_score, precision_score, recall_score
)
import matplotlib.pyplot as plt
import seaborn as sns
from copy import deepcopy

"""Downloading Datasets

###Datasets used: [Kaggle dataset bee or not bee](https://www.kaggle.com/datasets/chrisfilo/to-bee-or-no-to-bee)

###Git hub reference I used from the publisher itself [GIthub repository link from the publisher on the project used as reference](https://github.com/inesnolas/Audio_based_identification_beehive_states/blob/master/Bee_NotBee_classification/utils.py#L470)

###Research Paper I went for [Research Paper](https://zenodo.org/records/1321278/preview/Dataset%20documentation.pdf?include_deleted=0)
"""

import kagglehub
import os

# --- 1. Download the dataset using KaggleHub ---
# This 'path' variable now holds the root directory of your downloaded dataset.
base_data_directory = kagglehub.dataset_download("chrisfilo/to-bee-or-no-to-bee")
print("Dataset downloaded to:", base_data_directory)

# --- 2. Initialize dictionaries to store file paths ---
wav_files_found = {}  # {base_name: full_path_to_wav}
lab_files_found = {}  # {base_name: full_path_to_lab}
other_files_found = {} # {base_name: {ext: full_path}} - for .mp3, .mlf, etc.

# --- 3. Walk through the downloaded directory and categorize files ---
print("\n--- Scanning downloaded dataset for files ---")
for root, _, files in os.walk(base_data_directory):
    for file in files:
        base_name, ext = os.path.splitext(file)
        full_file_path = os.path.join(root, file)

        if ext.lower() == '.wav':
            wav_files_found[base_name] = full_file_path
        elif ext.lower() == '.lab':
            lab_files_found[base_name] = full_file_path
        else:
            if base_name not in other_files_found:
                other_files_found[base_name] = {}
            other_files_found[base_name][ext.lower()] = full_file_path

print("Scanning complete.")

"""###Data Exploratory"""

# --- 4. Map .wav and .lab files ---
# This dictionary will store pairs where both .wav and .lab exist for a given base_name
mapped_pairs = {}
unpaired_wav = []
unpaired_lab = []

for base_name in sorted(list(set(wav_files_found.keys()) | set(lab_files_found.keys()))):
    wav_path = wav_files_found.get(base_name)
    lab_path = lab_files_found.get(base_name)

    if wav_path and lab_path:
        mapped_pairs[base_name] = {'wav': wav_path, 'lab': lab_path}
    elif wav_path:
        unpaired_wav.append(wav_path)
    elif lab_path:
        unpaired_lab.append(lab_path)

print(f"\n--- Mapping Results ---")
print(f"Total .wav files found: {len(wav_files_found)}")
print(f"Total .lab files found: {len(lab_files_found)}")
print(f"Total mapped WAV-LAB pairs: {len(mapped_pairs)}")
print(f"Unpaired .wav files: {len(unpaired_wav)}")
print(f"Unpaired .lab files: {len(unpaired_lab)}")

if other_files_found:
    print(f"Other files found (excluding .wav/.lab): {len(other_files_found)} unique base names.")
    for base_name, exts_dict in list(other_files_found.items())[:5]: # Print first 5 for brevity
        print(f"  - {base_name}: {list(exts_dict.keys())}")


# --- Section 1: Displaying Mapped WAV-LAB Pairs ---
print("\n" + "="*50)
print("--- Section 1: Mapped WAV and LAB File Pairs ---")
print("="*50)

if mapped_pairs:
    for i, (base_name, paths) in enumerate(mapped_pairs.items()):
        print(f"Pair {i+1}:")
        print(f"  Base Name: {base_name}")
        print(f"  WAV Path: {paths['wav']}")
        print(f"  LAB Path: {paths['lab']}")
        if i >= 10: # Print only the first 10 pairs for brevity
            print(f"... and {len(mapped_pairs) - 10} more pairs.")
            break
else:
    print("No complete WAV-LAB pairs found with matching base names.")

from collections import defaultdict

# Updated versions to capture duplicates
wav_file_paths = defaultdict(list)
lab_file_paths = defaultdict(list)

# Detect duplicates
duplicate_wavs = {k: v for k, v in wav_file_paths.items() if len(v) > 1}
duplicate_labs = {k: v for k, v in lab_file_paths.items() if len(v) > 1}

print("\n--- Duplicate File Report ---")
print(f"Duplicate .wav base names: {len(duplicate_wavs)}")
for i, (base_name, paths) in enumerate(duplicate_wavs.items()):
    print(f"  {base_name}:")
    for path in paths:
        print(f"    - {path}")
    if i >= 5:
        print(f"  ... and {len(duplicate_wavs) - 5} more")
        break

print(f"Duplicate .lab base names: {len(duplicate_labs)}")
for i, (base_name, paths) in enumerate(duplicate_labs.items()):
    print(f"  {base_name}:")
    for path in paths:
        print(f"    - {path}")
    if i >= 5:
        print(f"  ... and {len(duplicate_labs) - 5} more")
        break

# --- Section 2: Displaying All .WAV Files ---
print("\n" + "="*50)
print("--- Section 2: All .WAV Files Found ---")
print("="*50)

if wav_files_found:
    for i, (base_name, wav_path) in enumerate(wav_files_found.items()):
        print(f"WAV File {i+1}:")
        print(f"  Base Name: {base_name}")
        print(f"  Path: {wav_path}")
        if i >= 10: # Print only the first 10 for brevity
            print(f"... and {len(wav_files_found) - 10} more WAV files.")
            break
else:
    print("No .wav files found.")

# --- Section 3: Displaying All .LAB Files ---
print("\n" + "="*50)
print("--- Section 3: All .LAB Files Found ---")
print("="*50)

if lab_files_found:
    for i, (base_name, lab_path) in enumerate(lab_files_found.items()):
        print(f"LAB File {i+1}:")
        print(f"  Base Name: {base_name}")
        print(f"  Path: {lab_path}")
        if i >= 10: # Print only the first 10 for brevity
            print(f"... and {len(lab_files_found) - 10} more LAB files.")
            break
else:
    print("No .lab files found.")

# --- Additional Section: Displaying Unpaired Files ---
print("\n" + "="*50)
print("--- Section 4: Unpaired Files ---")
print("="*50)

if unpaired_wav:
    print("\n--- Unpaired .WAV files (no matching .lab) ---")
    for i, path in enumerate(unpaired_wav):
        print(f"  {os.path.basename(path)}")
        if i >= 4: # Print first 5
            print(f"... and {len(unpaired_wav) - 5} more unpaired WAV files.")
            break
else:
    print("No unpaired .wav files found.")

if unpaired_lab:
    print("\n--- Unpaired .LAB files (no matching .wav) ---")
    for i, path in enumerate(unpaired_lab):
        print(f"  {os.path.basename(path)}")
        if i >= 4: # Print first 5
            print(f"... and {len(unpaired_lab) - 5} more unpaired LAB files.")
            break
else:
    print("No unpaired .lab files found.")

import os
import shutil

# Paths (change as needed)
wav_lab_folder = "path_to_save_wav_lab"  # where .wav + .lab go
other_files_folder = "path_to_save_other_files"  # where other files go

# Make sure output directories exist
os.makedirs(wav_lab_folder, exist_ok=True)
os.makedirs(other_files_folder, exist_ok=True)

# Your existing dictionaries from scanning
# wav_files_found, lab_files_found, other_files_found

# 1. Copy .wav and matching .lab files
for base_name in set(wav_files_found) | set(lab_files_found):
    wav_path = wav_files_found.get(base_name)
    lab_path = lab_files_found.get(base_name)

    if wav_path:
        shutil.copy2(wav_path, wav_lab_folder)
    if lab_path:
        shutil.copy2(lab_path, wav_lab_folder)

# 2. Copy other files
for base_name, exts_dict in other_files_found.items():
    for ext, file_path in exts_dict.items():
        shutil.copy2(file_path, other_files_folder)

print("Files copied successfully.")

"""The .lab extension files are annotated bee acoustic datasets

and the audio file of mp3 is splited and stored in the segments of .wav file extensions.

###Data Preprocessing
"""

def read_beeNotBee_annotations_saves_labels(audiofilename, block_name,  blockStart, blockfinish, annotations_path, threshold=0):


    ## function: reads corresponding annotation file (.lab) and assigns a label to one block/sample. Appends label into csv file.
    ##
    ## inputs:
    ## audiofilename = name of the audio file (no path), block_name = name of the sample/segment,  blockStart = time point in seconds where block starts, blockfinish = time point in seconds where block ends, annotations_path = path to annotations folder (where .lab files are), threshold = value tor threshold.
    ##
    ## outputs:
    ## label_th= 2 element list, [0] = a label (Queenbee / noQueenbee) for the block and threshold considered; [1] = label strength, value that reflects the proportion of noQueenbee interval in respect to the whole block.


    # threshold gives the minimum duration of the no bee intervals we want to consider.
    # threshold=0 uses every event as notBee whatever the duration
    # threshold=0.5 disregards intervals with less than half a second duration.

    block_length=blockfinish-blockStart

    if audiofilename.startswith('#'):
        annotation_filename=audiofilename[1:-4]+'.lab'
    else :
        annotation_filename=audiofilename[0:-4]+'.lab'


    try:
        with open(annotations_path + os.sep + annotation_filename,'r') as f:
            # EXAMPLE FILE:

            # CF003 - Active - Day - (223)
            # 0	8.0	bee
            # 8.01	15.05	nobee
            # 15.06	300.0	bee
            # .
            #

            # all files end with a dot followed by an empty line.

            print(annotations_path + os.sep + annotation_filename)
            lines = f.read().split('\n')

            labels_th=['bee', 0.0]
            label2assign='bee'
            label_strength=0
            intersected_s=0

            for line in lines:
                if (line == annotation_filename[0:-4]) or (line == '.') or (line ==''):
                    #ignores title, '.', or empty line on the file.
                    continue

                #print(line)
                parsed_line= line.split('\t')

                assert (len(parsed_line)==3), ('expected 3 fields in each line, got: '+str(len(parsed_line)))


                tp0=float(parsed_line[0])
                tp1=float(parsed_line[1])
                annotation_label=parsed_line[2]
                if blockfinish < tp0: # no need to read further nobee intervals since annotation line is already after block finishes
                    break

                if annotation_label== 'nobee':


                    if tp1-tp0 >= threshold:  # only progress if nobee interval is longer than defined threshold.

                        if tp0 > blockStart and tp0 <= blockfinish and tp1 >= blockfinish:

                            intersected_s=intersected_s + (blockfinish-tp0)
                            # |____________########|########
                            # bs          tp0      bf      tp1

                        elif tp1 >= blockStart and tp1 < blockfinish and tp0 <= blockStart:

                            intersected_s=intersected_s+ (tp1-blockStart)
                            # #####|########_____|
                            # tp0  bs     tp1    bf


                        elif tp1 >= blockStart and tp1 <= blockfinish and tp0 >= blockStart and tp0 <= blockfinish:

                            intersected_s=intersected_s+ (tp1-tp0)
                            # |_____########_____|
                            # bs   tp0    tp1    bf

                        elif tp0 <= blockStart and tp1 >= blockfinish:

                            intersected_s=intersected_s + (blockfinish-blockStart)
                            #  ####|############|####
                            # tp0  bs           bf  tp1

                    if intersected_s > 0:
                        label2assign='nobee'
                    label_strength= intersected_s/block_length # proportion of nobee length in the block


                    labels_th= [label2assign, round(label_strength,3)]  # if label_strehgth ==0 --> bee segment


            assert (blockfinish <=tp1 ), ('the end of the request block falls outside the file: block ending: '+ str(blockfinish)+' end of file at: '+ str(tp1))


    except FileNotFoundError as e:
        print(e, '--Anotation file does not exist! label as unknown')
        print(annotation_filename=audiofilename[0:-4]+'.lab')

        label2assign = 'unknown'
        label_strength=-1

        labels_th = [label2assign, label_strength]

    except Exception as e1:
        print('unknown exception: '+str(e1))
        quit


    return labels_th

def uniform_block_size(undersized_block, block_size_samples, method='repeat' ):

    lengthTofill=(block_size_samples)-(undersized_block.size)

    # Only pad if the block is actually undersized (lengthTofill is positive)
    if lengthTofill > 0:
        if method == 'zero_padding':
            new_block=np.pad(undersized_block, (0,lengthTofill), 'constant', constant_values=(0) )

        elif method=='mean_padding':
            new_block=np.pad(undersized_block, (0,lengthTofill), 'mean' )

        elif method=='repeat':
            new_block= np.pad(undersized_block, (0,lengthTofill), 'reflect')
        else:
            print('methods to choose are: \'zero_padding\' ,\'mean_padding\' and \'repeat\' ' )
            new_block=0
    else:
        # If not undersized, return the original block (or the first block_size_samples if it was slightly oversized)
        new_block = undersized_block[:block_size_samples]


    return new_block

import os
import glob

audiofilenames_list = [os.path.basename(f) for f in glob.glob(os.path.join("/content/path_to_save_wav_lab", '*.wav'))]

# Modified load_audioFiles_saves_segments function
def load_audioFiles_saves_segments( audiofilenames_list, path_audioFiles,path_save_audio_labels, block_size , thresholds, annotations_path, read_beeNotBee_annotations ='yes', save_audioSegments='yes'):

    print("Number of audiofiles in folder: "+str(len(audiofilenames_list)))

    fi=0
    for file_name in audiofilenames_list:
        fi=fi+1
        print('\n')
        print('Processing '+ file_name+'          :::file number:  '+str(fi)+' --------->of '+str(len(audiofilenames_list)))

        # Use soundfile to read the entire audio file first
        full_audio_path = path_audioFiles + file_name
        try:
            full_audio, sr = sf.read(full_audio_path)
            full_audio_duration = len(full_audio) / sr
            print(f"Successfully loaded full audio: {file_name}, duration: {full_audio_duration:.2f} seconds, sr: {sr}")
        except FileNotFoundError:
            print(f"Error: Audio file not found at {full_audio_path}. Skipping.")
            continue
        except Exception as e:
            print(f"Error loading audio file {full_audio_path}: {e}. Skipping.")
            continue


        offset_seconds=0
        block_id =0

        # Calculate total number of samples
        total_samples = len(full_audio)

        while offset_seconds * sr < total_samples:

            # Calculate start and end samples for the current block
            blockStart_samples = int(offset_seconds * sr)
            blockfinish_samples = int(min((offset_seconds + block_size) * sr, total_samples)) # Ensure we don't go past the end

            blockStart_seconds = offset_seconds
            blockfinish_seconds = blockfinish_samples / sr


            # Extract the block from the full audio data
            block = full_audio[blockStart_samples:blockfinish_samples]

            if block.shape[0] > 0:    #when total length = multiple of blocksize, results that last block is 0-lenght, this if bypasses those cases.

                block_name=file_name[0:-4]+'__segment'+str(block_id)
                print('-----------------Processing segment '+str(block_id) + ' (' + block_name + ')')


                # READ BEE NOT_BEE ANNOTATIONS:
                if read_beeNotBee_annotations == 'yes':
                    print('---------------------Will read BeeNotbee anotations and create labels for segment'+str(block_id))

                    for th in thresholds:

                        label_file_exists = os.path.isfile(path_save_audio_labels+'labels_BeeNotBee_th'+str(th)+'.csv')
                        with open(path_save_audio_labels+'labels_BeeNotBee_th'+str(th)+'.csv','a', newline='') as label_file:
                            writer =csv.DictWriter(label_file, fieldnames=['sample_name', 'segment_start','segment_finish', 'label_strength', 'label'], delimiter=',')
                            if not label_file_exists:
                                writer.writeheader()

                            label_block_th=read_beeNotBee_annotations_saves_labels(file_name, block_name,  blockStart_seconds, blockfinish_seconds, annotations_path, th)

                            writer.writerow({'sample_name': block_name, 'segment_start': blockStart_seconds, 'segment_finish': blockfinish_seconds , 'label_strength': label_block_th[1],  'label': label_block_th[0]} )
                            print('-----------------Wrote label for th '+ str(th)+' seconds of segment'+str(block_id)  )


                # MAKE BLOCK OF THE SAME SIZE:
                # Note: block_size is in seconds, we need block_size * sr in samples
                target_block_size_samples = int(block_size * sr)
                if block.shape[0] < target_block_size_samples:
                    block = uniform_block_size(block, target_block_size_samples, 'repeat')
                    print('-----------------Uniformizing block length of segment'+str(block_id)  )




                # Save audio segment
                if save_audioSegments == 'yes' and not os.path.exists(path_save_audio_labels + block_name + '.wav'):
                  try:
                    # Ensure data type is compatible with soundfile (e.g., float32)
                    block = block.astype(np.float32)
                    sf.write(path_save_audio_labels + block_name + '.wav', block, sr)
                    print('-----------------Saved wav file for segment ' + str(block_id))
                  except Exception as e:
                    print(f"Error saving segment {block_name}: {e}")

            else :
                print('----------------- no more segments for this file--------------------------------------')
                print('\n')
                break # Exit the while loop for this file

            # Move to the next segment
            offset_seconds += block_size
            block_id += 1
    print('______________________________No more audioFiles___________________________________________________')

    return

def read_HiveState_fromSampleName( filename, states):   #states: state_labels=['active','missing queen','swarm' ]
    label_state='other'
    for state in states:
        if state in filename.lower():
            label_state = state
    #incorporate condition for Nu-hive recordings which do not follow the same annotation: 'QueenBee' or 'NO_QueenBee'
    if label_state=='other':
        if 'NO_QueenBee' in filename:
            label_state = states[1]
        else:
            label_state=states[0]
    return label_state

def write_Statelabels_from_beeNotBeelabels(path_save, path_labels_BeeNotBee, states=['active','missing queen','swarm' ]):

    #label_file_exists = os.path.isfile(path_save+'state_labels.csv')

    with open(path_labels_BeeNotBee, 'r' ) as rfile, \
    open(path_save+'state_labels.csv', 'w', newline='') as f_out:
        csvreader = csv.reader(rfile, delimiter=',')
        writer= csv.DictWriter(f_out, fieldnames=['sample_name', 'label'], delimiter=',')

        #if not label_file_exists:
        writer.writeheader()

        for row in csvreader:
            if not row[0]=='sample_name':
                label_state=read_HiveState_fromSampleName(row[0], states)
                writer.writerow({'sample_name':row[0], 'label':label_state})
    return

def write_Statelabels_from_samplesFolder(path_save, path_samplesFolder, extension='.wav', states=['active','missing queen','swarm' ]):

    label_file_exists = os.path.isfile(path_save+'state_labels.csv')

    samples_names = get_list_samples_names(path_samplesFolder, extension)
    with open(path_save+'state_labels.csv', 'a', newline='') as f_out:

        writer= csv.DictWriter(f_out, fieldnames=['sample_name', 'label'], delimiter=',')

        if not label_file_exists:
            writer.writeheader()

        for name in samples_names:
            label_state = read_HiveState_fromSampleName(name, states)
            writer.writerow({'sample_name': name[0:-4] , 'label':label_state})
    return

def write_sample_ids_perHive(sample_ids , savepath):

  #identify different hives:
            #in the NU-Hive dataset the hives are identified in the string Hive1 or Hive3 in the beginning.
            #in OSBH every file referring to the same person will be considered as if the same Hive: identified by #nameInitials -
            # other files that do not follow this can be grouped in the same hive ()
            #get from unique filenames all unique identifiers of hives: either read the string until the first_  : example 'Hive3_
            #or get the string starting in '#' until the first ' - '. example: '#CF003 - '
    #uniqueFilenames=['Hive3_20_07_2017_QueenBee_H3_audio___15_40_00.wav',
    #                 'Hive1_20_07_2017_QueenBee_H3_audio___15_40_00.wav',
    #                 'Hive3_20_07_22017_QueenBee_H3_audio___15_40_00.wav',
    #                 'Sound Inside a Swarming Bee Hive  -25 to -15 minutes-sE02T8B2LfA.wav',
    #                 'Sound Inside a Swarming Bee Hive  +25 to -15 minutes-sE02T8B2LfA.wav',
    #                 '#CF003 - Active - Day - (222).csv', '#CF003 - Active - Day - (212).csv']


    uniqueHivesNames={}
    pat1=re.compile("(\w+\d)\s-\s")
    pat2=re.compile("^(Hive\d)_")
    pat3=re.compile("^(Hive\d)\s")
    for sample in sample_ids:

        match_pat1=pat1.match(sample)
        match_pat2=pat2.match(sample)
        match_pat3=pat3.match(sample)

        if match_pat1:
            if match_pat1.group(1) in uniqueHivesNames.keys():
                uniqueHivesNames[match_pat1.group(1)].append(sample)
            else:
                uniqueHivesNames[match_pat1.group(1)]=[sample]


        elif match_pat3:
            if match_pat3.group(1) in uniqueHivesNames.keys():
                uniqueHivesNames[match_pat3.group(1)].append(sample)
            else:
                uniqueHivesNames[match_pat3.group(1)]=[sample]

        elif match_pat2:
            if match_pat2.group(1) in uniqueHivesNames.keys():
                uniqueHivesNames[match_pat2.group(1)].append(sample)
            else:
                uniqueHivesNames[match_pat2.group(1)]=[sample]
        else:
            #odd case, like files names 'Sound Inside a Swarming Bee Hive  -25 to -15 minutes-sE02T8B2LfA.wav'
            #will be all gathred as the same hive, although we need to be careful if other names appear!
            if 'Sound Inside a Swarming Bee Hive' in uniqueHivesNames.keys():
                uniqueHivesNames['Sound Inside a Swarming Bee Hive'].append(sample)
            else:
                uniqueHivesNames['Sound Inside a Swarming Bee Hive']=[sample]



    with open(savepath+'sampleID_perHive.json', 'w') as outfile:
        json.dump(uniqueHivesNames, outfile)

    return uniqueHivesNames

def split_samples_byHive(test_size, train_size, hives_data_dictionary, splitPath_save):

    ## creates 3 different sets intended for hive-independent classification. meaning that samples are separated accordingly to the hive.
    ## input: test_size, ex: 0.1  : 10% hives for test
    ## train_size, ex: 0.7: 70% hives for training, 30% for valisdation. (after having selected test samples!!)
    ## splitPath_save = path and name where to save the splitted samples id dictionary

    ## output:
    ## returns and dumps a dictionary: {test : [sample_id1, sample_id2, ..], train : [], 'val': [sample_id2, sample_id2]}

    splittedSamples={'test': [], 'train': [], 'val':[]}

    n_hives = len(hives_data_dictionary.keys())

    hives_list=list(hives_data_dictionary.keys())

    hives_rest1=random.sample(hives_list, round(n_hives*(1-test_size)))

    if len(hives_rest1) == len(hives_list):
        rand_hive = random.sample(range(len(hives_rest1)),1)
        hives_rest=hives_rest1[:]
        del hives_rest[rand_hive[0]]
    else:

        hives_rest = hives_rest1[:]

    hiveTEST=np.setdiff1d(hives_list , hives_rest)
    hiveVAL=random.sample(hives_rest, round(len(hives_rest)*train_size))
    hiveTRAIN=np.setdiff1d(hives_rest , hiveVAL)


    print('hives for testing: '+ str(list(hiveTEST)))
    print('hives for training: '+ str(list(hiveTRAIN)))
    print('hives for validation: '+ str(hiveVAL))


    for ht in list(hiveTEST):
        splittedSamples['test'].extend(hives_data_dictionary[ht])

    for h1 in list(hiveTRAIN):
        splittedSamples['train'].extend(hives_data_dictionary[h1])

    for h2 in hiveVAL:
        splittedSamples['val'].extend(hives_data_dictionary[h2])

    with open(splitPath_save+'.json', 'w') as outfile:
        json.dump(splittedSamples, outfile)

    return splittedSamples

def get_days_from_sampleIds(sample_ids_same_hive):
    # Hive1 31_05_2018_NO_QueenBee____01_30_00.wav
    # Hive1 12_06_2018_QueenBee____16_10_00.wav
    # Hive3_28_07_2017_QueenBee____23_50_00.wav
    list_days_sampleIds={}

    for s in sample_ids_same_hive:
        if s[6:16] in list_days_sampleIds.keys():
            list_days_sampleIds[s[6:16]].append(s)
        else:
            list_days_sampleIds[s[6:16]] = [s]

    return list_days_sampleIds

def get_list_samples_names(path_audioSegments_folder, extension='.wav'):
    sample_ids=[os.path.basename(x) for x in glob.glob(path_audioSegments_folder+'*'+extension)]
    return sample_ids

def split_samples_random(test_size, path_audioSegments_folder, splitPath_save, extension='.wav'):
    """
    Splits samples randomly into train, validation, and test sets based on test_size and
    derives validation and train sizes to achieve approximately test_size / (remaining_size / 2) split.
    A typical use for 70/15/15 split would be test_size=0.15.
    """

    splittedSamples = {'test': [], 'train': [], 'val': []}

    list_samples_id = get_list_samples_names(path_audioSegments_folder, extension)
    n_segments = len(list_samples_id)

    if n_segments == 0:
        print("Warning: No samples found to split.")
        with open(splitPath_save + '.json', 'w') as outfile:
            json.dump(splittedSamples, outfile)
        return splittedSamples

    # Shuffle the list for randomness
    random.shuffle(list_samples_id)

    # Calculate sizes based on total samples
    n_test = round(n_segments * test_size)
    n_val = round(n_segments * test_size) # Use test_size again for validation (relative to total)
    n_train = n_segments - n_test - n_val

    # Ensure sizes are non-negative and sum correctly
    if n_train < 0:
        print("Warning: Calculated train size is negative. Adjusting sizes.")
        n_test = int(n_segments * 0.15)
        n_val = int(n_segments * 0.15)
        n_train = n_segments - n_test - n_val
        if n_train < 0: # Still negative? Fallback to simpler logic
             n_test = min(n_test, n_segments // 3)
             n_val = min(n_val, (n_segments - n_test) // 2)
             n_train = n_segments - n_test - n_val
             print(f"Adjusted split sizes: Train={n_train}, Val={n_val}, Test={n_test}")


    # Ensure we don't exceed list bounds
    n_test = min(n_test, n_segments)
    n_val = min(n_val, n_segments - n_test)
    n_train = n_segments - n_test - n_val # Recalculate train after min adjustments

    print(f"Total segments: {n_segments}")
    print(f"Calculated split sizes: Train={n_train}, Val={n_val}, Test={n_test}")


    # Perform the split using slicing after shuffling
    samplesTEST = list_samples_id[:n_test]
    samplesVAL = list_samples_id[n_test : n_test + n_val]
    samplesTRAIN = list_samples_id[n_test + n_val :]


    print('samples for testing: ' + str(len(samplesTEST)))
    print('samples for training: ' + str(len(samplesTRAIN)))
    print('samples for validation: ' + str(len(samplesVAL)))

    # Verify the total count
    if len(samplesTEST) + len(samplesVAL) + len(samplesTRAIN) != n_segments:
        print(f"Error: Split sample counts ({len(samplesTEST) + len(samplesVAL) + len(samplesTRAIN)}) do not sum to total segments ({n_segments})!")

    splittedSamples['val'] = samplesVAL
    splittedSamples['test'] = samplesTEST
    splittedSamples['train'] = samplesTRAIN

    with open(splitPath_save + '.json', 'w') as outfile:
        json.dump(splittedSamples, outfile)

    return splittedSamples

"""###Feature Extraction Function"""

def raw_feature_fromSample(path_audio_samples, sample_name, feature2extract):
    full_audio_path = os.path.join(path_audio_samples, sample_name)
    audio_sample, sr = librosa.load(full_audio_path)

    # Extract number from feature2extract (e.g., MFCCs20 → 20)
    match = re.search(r"(\d+)", feature2extract)
    if not match:
        raise ValueError(f"Cannot extract frequency count from feature2extract: {feature2extract}")

    n_freqs = int(match.group(1))

    # Compute the STFT (spectrogram)
    S = np.abs(librosa.stft(audio_sample))**2

    # Feature extraction
    if 'LOG' in feature2extract:
        Melspec = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=n_freqs)
        x = librosa.power_to_db(Melspec + 1)

    elif 'MFCCs' in feature2extract:
        Melspec = librosa.feature.melspectrogram(S=S, sr=sr)
        x = librosa.feature.mfcc(S=librosa.power_to_db(Melspec), sr=sr, n_mfcc=n_freqs)

    else:
        # fallback to mel spec only
        x = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=n_freqs)

    return x

def raw_feature_fromSample_withAUGMENTATION( sample_name, path_audio_sample, list_features ,augmentingFactor):


    audio_sample_or, sr = librosa.core.load(path_audio_sample + sample_name)
    # list_audio_samples = [audio_sample_or]
    spectrogram_original =[]

    dict_augmented_wav_feature = {}


    pitch = muda.deformers.LinearPitchShift(n_samples=augmentingFactor, lower=-1, upper=1)
    jam=jams.JAMS()
    j_orig = muda.jam_pack(jam, _audio=dict(y=audio_sample_or, sr=sr))

    for i, jam_out in enumerate(pitch.transform(j_orig)):
        y = jam_out.sandbox.muda._audio['y']
        sr = jam_out.sandbox.muda._audio['sr']

        #list_audio_samples.append(y)  #list with original_audio plus every modified version of the same audio
        dict_augmented_wav_feature[sample_name[0:-4]+'_AG'+str(i)]=[y]

    # Extract features for the set of samples: original and  augmented...
    for feature2extract in list_features:
        m = re.match(r"\w+s(\d+)", feature2extract)
        n_freqs=int(m.groups()[0])


        # EXTRACT FOR ORIGINAL SAMPLE:
        Melspec = librosa.feature.melspectrogram(audio_sample_or, n_mels = n_freqs) # computes mel spectrograms from audio sample,

        if 'LOG' in feature2extract: #'LOG_MELfrequencies48'
            Melspec=librosa.feature.melspectrogram(audio_sample_or, sr=sr, n_mels=n_freqs)
            x=librosa.power_to_db(Melspec+1)

        elif 'MFCCs' in feature2extract:
            n_freqs = int(feature2extract[5:len(feature2extract)])
            Melspec = librosa.feature.melspectrogram(audio_sample_or, sr=sr)
            x = librosa.feature.mfcc(S=librosa.power_to_db(Melspec),sr=sr, n_mfcc = n_freqs)

        else:
            x = Melspec
        spectrogram_original.append(x)   # append each feature!


        # EXTRACT FOR SET OF AUGMENTED SAMPLES:

        for aug_audio_sample_name in dict_augmented_wav_feature.keys():


            Melspec = librosa.feature.melspectrogram(dict_augmented_wav_feature[aug_audio_sample_name][0], n_mels = n_freqs) # computes mel spectrograms from audio sample,

            if 'LOG' in feature2extract: #'LOG_MELfrequencies48'
                Melspec=librosa.feature.melspectrogram(dict_augmented_wav_feature[aug_audio_sample_name][0], sr=sr, n_mels=n_freqs)
                x=librosa.power_to_db(Melspec+1)

            elif 'MFCCs' in feature2extract:
                n_freqs = int(feature2extract[5:len(feature2extract)])
                Melspec = librosa.feature.melspectrogram(dict_augmented_wav_feature[aug_audio_sample_name][0], sr=sr)
                x = librosa.feature.mfcc(S=librosa.power_to_db(Melspec),sr=sr, n_mfcc = n_freqs)

            else:
                x = Melspec

            dict_augmented_wav_feature[aug_audio_sample_name].append(x)    #dict; {sample_name_AG1 : [y,spectrogram], sample_name_AG2 : [y,spectrogram]}



    return dict_augmented_wav_feature, spectrogram_original

def compute_statistics_overSpectogram(spectrogram):

    x_diff=np.diff(spectrogram,1,0)

    X_4features=np.concatenate((np.mean(spectrogram,1), np.std(spectrogram,1),np.mean(x_diff,1), np.std(x_diff,1)), axis=0)

    X_flat = np.asarray(X_4features)

    return X_flat

def compute_statistics_overMFCCs(MFCC, first='yes'):

    x_delta=librosa.feature.delta(MFCC)
    x_delta2=librosa.feature.delta(MFCC, order=2)

    if first=='no':
        MFCC=MFCC[1:]
        x_delta=x_delta[1:]
        x_delta2=x_delta2[1:]

    X_4features=np.concatenate((np.mean(MFCC,1), np.std(MFCC,1),np.mean(x_delta,1), np.std(x_delta,1), np.mean(x_delta2,1), np.std(x_delta2,1)), axis=0)

    X_flat = np.asarray(X_4features)

    return X_flat

def get_samples_id_perSet(pathSplitFile):  # reads split_id file


    split_dict=json.load(open (pathSplitFile, 'r'))

    sample_ids_test = split_dict['test']
    sample_ids_train = split_dict['train']
    sample_ids_val = split_dict['val']
    return sample_ids_test, sample_ids_train, sample_ids_val

def get_features_from_samples(path_audio_samples, sample_ids, raw_feature, normalization, high_level_features ): #normalization = NO, z_norm, min_max
    ## function to extract features
    #high_level_features = 0 or 1

    n_samples_set = len(sample_ids) # 4
    feature_Maps = []

    for sample in sample_ids:

        # raw feature extraction:
        x = raw_feature_fromSample( path_audio_samples, sample, raw_feature ) # x.shape: (4, 20, 2584)


        ##normalization here:
        # if not normalization == 'NO':
            # x_norm = featureMap_normalization_block_level(x, normalizationType = normalization)
        # else: x_norm = x

        if high_level_features:
            # high level feature extraction:
            if 'MFCCs' in raw_feature:
                X = compute_statistics_overMFCCs(x, 'yes') # X.shape: (4 , 120)
            else:
                X = compute_statistics_overSpectogram(x)

            feature_map=X
        else:
            feature_map=x


        feature_Maps.append(feature_map)

    return feature_Maps

def get_GT_labels_fromFiles(path_labels, sample_ids, labels2read) : #labels2read =  name of the label file

    ##reads labels files and retreives labels for a sample set given by sample_ids
    # input:  path_labels: where label file is
    #         sample_ids: list of sample names that we want the label
    #         labels2read: name of the labels file: state_labels, labels_BeeNotBee_th0 ...

    # output: list of string labels, in same order as sample_ids list

    labels = []
    fileAsdict={}

    # Use os.path.join to correctly construct the full file path
    full_label_filepath = os.path.join(path_labels, labels2read + '.csv')

    try:
        with open(full_label_filepath, 'r') as labfile:
            csvreader = csv.reader(labfile, delimiter=',')
            for row in csvreader:
                if not row[0] == 'sample_name':
                    fileAsdict[row[0]]=row[-1]   # row[-1] = '/missing queen/active' or 'bee/nobee'

        for sample in sample_ids:
            # Ensure we handle cases where the sample name might not be in the dictionary
            # This could happen if there's a mismatch between segmented audio files and labels
            sample_base_name = sample[0:-4] # remove .wav extension
            if sample_base_name in fileAsdict:
                 labels.append(fileAsdict[sample_base_name])
            else:
                 print(f"Warning: Label not found for sample: {sample}. Skipping or assigning default.")
                 labels.append('unknown')


    except FileNotFoundError:
        print(f"Error: Label file not found at {full_label_filepath}")
        raise

    except Exception as e:
        print(f"An error occurred while reading {full_label_filepath}: {e}")
        # Handle other potential errors during file reading or processing
        raise


    return labels

def labels2binary(pos_label, list_labels):  # pos_label = missing queen / nobee
    list_binary_labels=[]
    for l in list_labels:
        if l == pos_label:
            list_binary_labels.append(1)
        else:
            list_binary_labels.append(0)
    return list_binary_labels

"""###SVM Classification"""

def SVM_Classification_inSplittedSets(X_flat_train, Y_train, X_flat_test, Y_test, kerneloption='rbf'):


    print('Starting classification with SVM:')
    Test_Preds=[]
    Train_Preds=[]
    Test_Preds_Proba=[]
    Train_Preds_Proba=[]
    Test_GroundT=[]
    Train_GroundT=[]
    CLFs=[]


    #train :
    clf = svm.SVC(kernel=kerneloption, probability=True)
    clf.fit(X_flat_train, Y_train)
    y_pred_train = clf.predict(X_flat_train)  # get binary predictions
    y_pred_proba_train = clf.predict_proba(X_flat_train) # get probability predictions


    # test:
    y_pred_test = clf.predict(X_flat_test)  # get binary predictions
    y_pred_proba_test = clf.predict_proba(X_flat_test)  # get probability predictions

    return clf, y_pred_proba_train, y_pred_train, y_pred_proba_test, y_pred_test

"""###Calling the functions for Data Processing"""

# Install necessary libraries
!pip install pydub pandas --quiet
print("Libraries 'pydub' and 'pandas' installed.")

from pydub import AudioSegment
from pydub.utils import mediainfo # To get audio duration

# --- RUN THIS CELL FIRST: Environment Setup, Install Libraries & Create utils.py ---

# Set environment variable to disable Numba JIT for Librosa, which often resolves TypingErrors
# If you encounter the error again, try 'os.environ['NUMBA_DISABLE_JIT'] = '1'' instead
os.environ['LIBROSA_NO_NUMBA'] = '1'
print("Environment variable LIBROSA_NO_NUMBA set to '1'.")

def main():
    print("Executing main function...")
    block_size = 60
    thresholds = [0, 5]
    # Ensure these paths are correct based on your KaggleHub download location
    # And that they have trailing slashes for directory paths
    path_audioFiles = "/content/path_to_save_wav_lab/"  # path to audio files
    annotations_path = "/content/path_to_save_wav_lab/" # path to .lab files
    path_save_audio_labels = '/content/dataset_BeeNoBee_' + str(block_size) + 'sec/' # Path for segmented audio and labels

    print(f"Block size: {block_size}")
    print(f"Thresholds: {thresholds}")
    print(f"Source audio/lab path: {path_audioFiles}")
    print(f"Output segmented audio/labels path: {path_save_audio_labels}")

    # Ensure output directory exists
    if not os.path.exists(path_save_audio_labels):
        print(f"Output directory {path_save_audio_labels} does not exist. Creating...")
        os.makedirs(path_save_audio_labels)
        print(f"Created directory: {path_save_audio_labels}")
    else:
        print(f"Output directory {path_save_audio_labels} already exists.")

    # Get list of audio filenames from the source directory
    # This line was in a separate cell before, ensure it's executed before load_audioFiles_saves_segments
    audiofilenames_list = [os.path.basename(f) for f in glob.glob(os.path.join(path_audioFiles, '*.wav'))]
    print(f"Found {len(audiofilenames_list)} .wav files in {path_audioFiles}")
    if not audiofilenames_list:
        print("Warning: No .wav files found in the source directory. load_audioFiles_saves_segments might not process anything.")
        # Consider exiting or handling this case

    # segments audio files, assigns label BeeNotBee to each block, writes labels to csv , saves segmened blocks in wav.
    # independently of flag save_audioSegments, if .wav with same name already exists it won't save again.
    # new labels are just appended to existing labels file, if purpose is to redo the whole file delete before running.
    print("--- Attempting to call load_audioFiles_saves_segments ---")
    # Pass the list of filenames
    load_audioFiles_saves_segments( audiofilenames_list, path_audioFiles, path_save_audio_labels, block_size , thresholds, annotations_path, read_beeNotBee_annotations='yes', save_audioSegments='yes')
    print("--- Returned from load_audioFiles_saves_segments ---")


    # --- Add tracing prints inside load_audioFiles_saves_segments function as well ---
    # (You would need to modify the load_audioFiles_saves_segments function definition itself)
    # For example, add print("Inside load_audioFiles_saves_segments") at the beginning
    # and print("Processing file:", file_name) inside the loop.

    path_beeNotbee_labels = os.path.join(path_save_audio_labels, 'labels_BeeNotBee_th'+str(thresholds[0])+'.csv')
    print(f"Expected label file path for writing state labels: {path_beeNotbee_labels}")

    # Add a check to ensure the label file exists before trying to read it
    if not os.path.exists(path_beeNotbee_labels):
        print(f"Error: Label file NOT found at {path_beeNotbee_labels}")
        print("Execution stopped before calling write_Statelabels_from_beeNotBeelabels.")
        print("Please inspect the output above. If 'Returned from load_audioFiles_saves_segments' is printed, check for errors logged during its execution.")
        print("If '--- Attempting to call load_audioFiles_saves_segments ---' is the last print, something is preventing the function call itself.")
        return # Stop execution if the file is missing
    else:
        print(f"Confirmed: Label file found at {path_beeNotbee_labels}")


    # reads labels beeNotBee files and creates corresponding states label file.
    print("--- Attempting to call write_Statelabels_from_beeNotBeelabels ---")
    write_Statelabels_from_beeNotBeelabels(path_save_audio_labels, path_beeNotbee_labels, states=['active','missing queen','swarm' ])
    print("--- Returned from write_Statelabels_from_beeNotBeelabels ---")


    sample_ids = get_list_samples_names(path_save_audio_labels) # get sample ids from audio segments folder.
    print(f"Found {len(sample_ids)} segmented audio files in {path_save_audio_labels}")


    # split data by Hive
    print("--- Attempting to call write_sample_ids_perHive ---")
    hives = write_sample_ids_perHive(sample_ids , path_save_audio_labels)  # retrieves unique hives names and also writes these to a file
    print("--- Returned from write_sample_ids_perHive ---")
    #hives=get_uniqueHives_names_from_File(path_save_audio_labels)
    print(f"Identified {len(hives)} unique hives.")


    print("--- Attempting to call split_samples_byHive ---")
    for i in range(3):
        print(f"  Splitting by hive, iteration {i+1}...")
        split_dict = split_samples_byHive(0.1, 0.5, hives, path_save_audio_labels+'split_byHive_'+str(i))
        print(f"    Split {i+1}: Train={len(split_dict['train'])}, Val={len(split_dict['val'])}, Test={len(split_dict['test'])} samples.")
    print("--- Returned from split_samples_byHive loop ---")


    #split data in 70:15:15 ratio
    print("--- Attempting to call split_samples_random ---")
    for i in range(3):
      print(f"  Splitting randomly, iteration {i+1}...")
      split_dict = split_samples_random(test_size=0.15,path_audioSegments_folder=path_save_audio_labels, splitPath_save=path_save_audio_labels+'split_random_'+str(i))
      print(f"    Split {i+1}: Train={len(split_dict['train'])}, Val={len(split_dict['val'])}, Test={len(split_dict['test'])} samples.")
    print("--- Returned from split_samples_random loop ---")

if __name__ == "__main__":
    main()

"""###Using SVM Classifier"""

def main():
   #----------------------------------- parameters to change-----------------------------------#
    path_workingFolder='/content/dataset_BeeNoBee_60sec'  # path where to saved audio segments and labels files.
    labels2read= 'labels_BeeNotBee_th0'
    feature = 'MFCCs20'
   #-------------------------------------------------------------------------------------------#

    # Ensure labels2read is just the filename without the path
    labels2read = os.path.basename(labels2read)

#Feature extraction:
    sample_ids_test, sample_ids_train, sample_ids_val = get_samples_id_perSet('/content/dataset_BeeNoBee_60sec/split_random_0.json')

    print(f"Number of training samples IDs: {len(sample_ids_train)}")
    print(f"Number of validation samples IDs: {len(sample_ids_val)}")
    print(f"Number of test samples IDs: {len(sample_ids_test)}")

    X_train = get_features_from_samples(path_workingFolder, sample_ids_train, feature, 'NO', 1)
    X_val = get_features_from_samples(path_workingFolder, sample_ids_val, feature, 'NO', 1)
    X_test = get_features_from_samples(path_workingFolder, sample_ids_test, feature, 'NO', 1)

    # get_features_from_samples returns a list of arrays, concatenate them into a single numpy array
    X_train = np.array(X_train)
    X_val = np.array(X_val)
    X_test = np.array(X_test)

    print(f"Shape of X_train features: {X_train.shape}")
    print(f"Shape of X_val features: {X_val.shape}")
    print(f"Shape of X_test features: {X_test.shape}")


    labels_train = get_GT_labels_fromFiles(path_workingFolder, sample_ids_train, labels2read)
    Y_train= labels2binary('nobee', labels_train)

    labels_val = get_GT_labels_fromFiles(path_workingFolder, sample_ids_val, labels2read)
    Y_val= labels2binary('nobee', labels_val)

    labels_test = get_GT_labels_fromFiles(path_workingFolder, sample_ids_test, labels2read)
    Y_test= labels2binary('nobee', labels_test)

    # Convert label lists to numpy arrays for concatenation
    Y_train = np.array(Y_train)
    Y_val = np.array(Y_val)
    Y_test = np.array(Y_test)

    print(f"Shape of Y_train labels: {Y_train.shape}")
    print(f"Shape of Y_val labels: {Y_val.shape}")
    print(f"Shape of Y_test labels: {Y_test.shape}")

    # Concatenate training and validation features and labels
    X_flat_train=np.concatenate( (X_train , X_val), axis = 0)
    Y_flat_train = np.concatenate( (Y_train, Y_val), axis = 0) # Concatenate labels as well

    print(f"Shape of combined training+validation features (X_flat_train): {X_flat_train.shape}")
    print(f"Shape of combined training+validation labels (Y_flat_train): {Y_flat_train.shape}")

    # Check if the number of samples is consistent before fitting
    if X_flat_train.shape[0] != Y_flat_train.shape[0]:
        print("Error: Inconsistent number of samples between combined features and labels.")
        print(f"X_flat_train samples: {X_flat_train.shape[0]}")
        print(f"Y_flat_train samples: {Y_flat_train.shape[0]}")
        return


    clf, y_pred_proba_train, y_pred_train, y_pred_proba_test, y_pred_test= SVM_Classification_inSplittedSets(X_flat_train, Y_flat_train, X_test, Y_test, kerneloption='rbf')

    Y_scores = clf.predict_proba(X_test)[:, 1]  # Probabilities for class 1

    # Binary predictions
    Y_pred = clf.predict(X_test)

    # --- Core Metrics ---
    accuracy = accuracy_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred, zero_division=0)
    recall = recall_score(Y_test, Y_pred, zero_division=0)
    f1 = f1_score(Y_test, Y_pred, zero_division=0)
    auroc = roc_auc_score(Y_test, Y_scores)
    auprc = average_precision_score(Y_test, Y_scores)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(f"AU-ROC: {auroc:.4f}")
    print(f"AU-PRC: {auprc:.4f}")

    # --- Confusion Matrix ---
    cm = confusion_matrix(Y_test, Y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["nobee", "bee"], yticklabels=["nobee", "bee"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

    # --- ROC Curve ---
    fpr, tpr, _ = roc_curve(Y_test, Y_scores)
    plt.figure()
    plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auroc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.grid(True)
    plt.show()

   # --- Precision-Recall Curve ---
    precision_vals, recall_vals, _ = precision_recall_curve(Y_test, Y_scores)
    plt.figure()
    plt.plot(recall_vals, precision_vals, label=f"PRC (AUC = {auprc:.2f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()

"""###Using CRNN Model

requirements.txt file
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

"""CRNN Architecture"""

class CRNN(nn.Module):
    def __init__(self, n_freqs: int, hidden_size: int = 64):
        """
        A simple CRNN model with 2D convolutions, followed by a GRU and fully connected output.

        Args:
            n_freqs (int): Number of input frequency bins (e.g., MFCCs or Mel).
            hidden_size (int): Hidden size of the GRU layer.
        """
        super(CRNN, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(2)

        # After 2 poolings, the freq dimension is reduced by 4
        self.rnn = nn.GRU(input_size=(n_freqs // 4) * 32, hidden_size=hidden_size, batch_first=True)

        # Fully connected layer to output binary prediction
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        """
        Forward pass of CRNN.

        Args:
            x (torch.Tensor): Input tensor of shape (B, 1, n_freqs, T)

        Returns:
            torch.Tensor: Output probabilities of shape (B, 1)
        """
        # Apply convolution + pooling
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))  # (B, 16, n_freqs/2, T/2)
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))  # (B, 32, n_freqs/4, T/4)

        B, C, freq_bins, T = x.shape

        # Prepare for RNN: (B, T, features)
        x = x.permute(0, 3, 1, 2).contiguous()  # (B, T, C, freq_bins)
        x = x.view(B, T, -1)                    # (B, T, C * freq_bins)

        # RNN
        x, _ = self.rnn(x)                      # (B, T, hidden_size)
        x = x[:, -1]                            # Take last time step: (B, hidden_size)

        # Output
        x = self.fc(x)                          # (B, 1)
        return torch.sigmoid(x)

class AudioSegmentDataset(Dataset):
    """
    PyTorch Dataset for audio segments and binary labels.

    Args:
        sample_ids: list of filenames (e.g., ['Hive1_segment0.wav', ...]).
        labels:  list or array of binary labels (0 or 1), same order as sample_ids.
        path_audio: directory where the .wav files live.
        feature: feature descriptor, passed to raw_feature_fromSample (e.g. 'MFCCs20').
        max_len: maximum number of time frames; shorter sequences will be padded, longer truncated.
    """
    def __init__(self, sample_ids, labels, path_audio, feature, max_len=200):
        self.ids = sample_ids
        self.labels = labels
        self.path_audio = path_audio
        self.feature = feature
        self.max_len = max_len

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        fname = self.ids[idx]
        x = raw_feature_fromSample(self.path_audio, fname, self.feature)
        # x has shape (n_freqs, T)
        n_freqs, T = x.shape

        # Pad or truncate time axis
        if T < self.max_len:
            pad_width = self.max_len - T
            x = np.pad(x, ((0,0),(0,pad_width)), mode='constant')
        else:
            x = x[:, :self.max_len]

        # Convert to tensor with shape (1, n_freqs, max_len)
        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)
        y = torch.tensor(self.labels[idx], dtype=torch.float32)
        return x, y

"""###Training and Evaluation Loop"""

def train_crnn(model,
               train_ds,
               val_ds,
               lr: float = 5e-4,
               batch_size: int = 32,
               n_epochs: int = 30,
               patience: int = 8):
    """
    Trains a CRNN model on audio segment data with early stopping and LR scheduling.

    Args:
        model (torch.nn.Module): The CRNN model to train.
        train_ds (Dataset): PyTorch Dataset for training data.
        val_ds (Dataset): PyTorch Dataset for validation data.
        lr (float): Initial learning rate.
        batch_size (int): Batch size.
        n_epochs (int): Maximum number of training epochs.
        patience (int): Early stopping patience.

    Returns:
        torch.nn.Module: Trained model with best validation loss.
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    criterion = nn.BCEWithLogitsLoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size)

    best_val_loss = float('inf')
    best_state_dict = None
    epochs_no_improve = 0

    for epoch in range(1, n_epochs + 1):
        model.train()
        running_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)

            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
            optimizer.step()

            running_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_val, y_val in val_loader:
                X_val, y_val = X_val.to(device), y_val.to(device).unsqueeze(1)
                outputs = model(X_val)
                val_loss += criterion(outputs, y_val).item()
        val_loss /= len(val_loader)
        scheduler.step(val_loss)

        print(f"Epoch {epoch}: Train Loss = {running_loss / len(train_loader):.4f}, Val Loss = {val_loss:.4f}")

        # Early Stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state_dict = model.state_dict()
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print(f"Early stopping triggered after {epoch} epochs.")
                break

    model.load_state_dict(best_state_dict)
    return model

"""Integrating the main function"""

def main():
    #Load your splits and features as before
    sample_test, sample_train, sample_val = get_samples_id_perSet('/content/dataset_BeeNoBee_60sec/split_random_0.json')
    path = '/content/dataset_BeeNoBee_60sec/'
    labels2read = 'labels_BeeNotBee_th0'
    feature = 'MFCCs20'

    # Get ground-truth labels
    Y_train = labels2binary('nobee', get_GT_labels_fromFiles(path, sample_train, labels2read))
    Y_val   = labels2binary('nobee', get_GT_labels_fromFiles(path, sample_val,   labels2read))
    Y_test  = labels2binary('nobee', get_GT_labels_fromFiles(path, sample_test,  labels2read))

    # Build datasets (choose max_len based on longest MFCC time axis in your data)
    max_len = 200
    train_ds = AudioSegmentDataset(sample_train, Y_train, path, feature, max_len)
    val_ds   = AudioSegmentDataset(sample_val,   Y_val,   path, feature, max_len)
    test_ds  = AudioSegmentDataset(sample_test,  Y_test,  path, feature, max_len)

    # Instantiate CRNN (n_freqs=20 in your MFCCs20 case)
    model = CRNN(n_freqs=20)

    # Train with early stopping
    model = train_crnn(model, train_ds, val_ds,
                       lr=1e-3, batch_size=32, n_epochs=30, patience=5)

    # Evaluate on test set (reuse your metric code)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)
    all_preds = []
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(device)  # shape: (B, 1, n_mfcc, time)
            y = y.to(device)

            logits = model(x)
            probs = torch.sigmoid(logits).squeeze()
            preds = (probs > 0.5).long()

            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    # Convert to numpy arrays
    Y_pred = np.array(all_preds)
    Y_scores = np.array(all_probs)
    Y_true = np.array(all_labels)

    # --- Core Metrics ---
    accuracy = accuracy_score(Y_true, Y_pred)
    precision = precision_score(Y_true, Y_pred, zero_division=0)
    recall = recall_score(Y_true, Y_pred, zero_division=0)
    f1 = f1_score(Y_true, Y_pred, zero_division=0)
    auroc = roc_auc_score(Y_true, Y_scores)
    auprc = average_precision_score(Y_true, Y_scores)

    print("\nTest set evaluation:")
    print(f"  Accuracy : {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall   : {recall:.4f}")
    print(f"  F1-score : {f1:.4f}")
    print(f"  AU-ROC   : {auroc:.4f}")
    print(f"  AU-PRC   : {auprc:.4f}")

    # --- Confusion Matrix ---
    cm = confusion_matrix(Y_true, Y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["nobee", "bee"], yticklabels=["nobee", "bee"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix (Test Set)")
    plt.show()

    # --- ROC Curve ---
    fpr, tpr, _ = roc_curve(Y_true, Y_scores)
    plt.figure()
    plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auroc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve (Test Set)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # --- Precision-Recall Curve ---
    precision_vals, recall_vals, _ = precision_recall_curve(Y_true, Y_scores)
    plt.figure()
    plt.plot(recall_vals, precision_vals, label=f"PRC (AUC = {auprc:.2f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve (Test Set)")
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()

"""**SVM Classifier Evaluation:**

*   **Accuracy:** 0.7416
*   **Precision:** 0.7783
*   **Recall:** 0.6881
*   **F1-score:** 0.7305
*   **AU-ROC:** 0.8257
*   **AU-PRC:** 0.8182

**CRNN Model Evaluation:**

*   **Accuracy:** 0.8093
*   **Precision:** 0.7831
*   **Recall:** 0.8549
*   **F1-score:** 0.8174
*   **AU-ROC:** 0.8944
*   **AU-PRC:** 0.8826

**Summary and Comparison:**

Both models perform reasonably well on the classification task. However, the **CRNN model is the best model** based on the reported evaluation metrics on the test set.

Here's why:

1.  **Higher Overall Performance:** The CRNN model consistently shows better performance across most metrics compared to the SVM classifier.
2.  **Accuracy:** The CRNN has a higher accuracy (0.8093 vs 0.7416), meaning it correctly classifies a larger proportion of samples.
3.  **Recall (Sensitivity):** The CRNN has significantly higher recall (0.8549 vs 0.6881). This is particularly important if identifying the positive class ('nobee' in this case) is crucial, as it means the CRNN is much better at finding all the actual 'nobee' instances.
4.  **F1-score:** The F1-score (harmonic mean of precision and recall) is higher for the CRNN (0.8174 vs 0.7305), indicating a better balance between precision and recall.
5.  **AU-ROC and AU-PRC:** The Area Under the ROC Curve (AU-ROC) and Area Under the Precision-Recall Curve (AU-PRC) are both substantially higher for the CRNN (0.8944 vs 0.8257 for AU-ROC, and 0.8826 vs 0.8182 for AU-PRC). These metrics are good overall indicators of a classifier's performance across different thresholds, and higher values suggest the CRNN is better at distinguishing between the two classes.
6.  **Precision:** While the precision of the CRNN (0.7831) is very similar to the SVM (0.7783), its recall is significantly higher.

In summary, the CRNN model demonstrates superior performance on the test set, particularly in its ability to identify positive instances (high recall) while maintaining good precision, leading to better overall classification performance as indicated by its higher F1-score, AU-ROC, and AU-PRC.

###Using CNN Model
"""

class CNN(nn.Module):
    def __init__(self, n_freqs):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.bn1   = nn.BatchNorm2d(16)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2   = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(2)

        self.flatten = nn.Flatten()
        self.fc = nn.Linear(32 * (n_freqs // 4) * (200 // 4), 1)  # Adjust 200 to your max_len
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.flatten(x)
        x = self.fc(x)
        return self.sigmoid(x)

"""
 Trying: LR = 0.001, Batch Size = 32
  Epoch 1: Train Loss = 0.5070, Val Loss = 0.5385, F1 = 0.9106
  Epoch 2: Train Loss = 0.4881, Val Loss = 0.5386, F1 = 0.9106
  Epoch 3: Train Loss = 0.4858, Val Loss = 0.5253, F1 = 0.9106
  Epoch 4: Train Loss = 0.4880, Val Loss = 0.5264, F1 = 0.9106
  Epoch 5: Train Loss = 0.4826, Val Loss = 0.5268, F1 = 0.9106
  Epoch 6: Train Loss = 0.4817, Val Loss = 0.5277, F1 = 0.9106
  Epoch 7: Train Loss = 0.4824, Val Loss = 0.5290, F1 = 0.8739
  Epoch 8: Train Loss = 0.4802, Val Loss = 0.5270, F1 = 0.9106
Early stopping

 Trying: LR = 0.001, Batch Size = 64
  Epoch 1: Train Loss = 0.5309, Val Loss = 0.5581, F1 = 0.9106
  Epoch 2: Train Loss = 0.5153, Val Loss = 0.5581, F1 = 0.9106
  Epoch 3: Train Loss = 0.5158, Val Loss = 0.5581, F1 = 0.9106
  Epoch 4: Train Loss = 0.5178, Val Loss = 0.5581, F1 = 0.9106
  Epoch 5: Train Loss = 0.5147, Val Loss = 0.5581, F1 = 0.9106
  Epoch 6: Train Loss = 0.5158, Val Loss = 0.5581, F1 = 0.9106
  Epoch 7: Train Loss = 0.5153, Val Loss = 0.5581, F1 = 0.9106
  Epoch 8: Train Loss = 0.5147, Val Loss = 0.5581, F1 = 0.9106
Early stopping

 Trying: LR = 0.0005, Batch Size = 32
  Epoch 1: Train Loss = 0.5250, Val Loss = 0.5286, F1 = 0.9106
  Epoch 2: Train Loss = 0.5163, Val Loss = 0.5285, F1 = 0.9106
  Epoch 3: Train Loss = 0.5150, Val Loss = 0.5285, F1 = 0.9106
  Epoch 4: Train Loss = 0.5126, Val Loss = 0.5285, F1 = 0.9106
  Epoch 5: Train Loss = 0.5150, Val Loss = 0.5285, F1 = 0.9106
  Epoch 6: Train Loss = 0.5126, Val Loss = 0.5285, F1 = 0.9106
  Epoch 7: Train Loss = 0.5150, Val Loss = 0.5285, F1 = 0.9106
  Epoch 8: Train Loss = 0.5175, Val Loss = 0.5285, F1 = 0.9106
Early stopping

 Trying: LR = 0.0005, Batch Size = 64
  Epoch 1: Train Loss = 0.5318, Val Loss = 0.5582, F1 = 0.9106
  Epoch 2: Train Loss = 0.5147, Val Loss = 0.5581, F1 = 0.9106
  Epoch 3: Train Loss = 0.5158, Val Loss = 0.5581, F1 = 0.9106
  Epoch 4: Train Loss = 0.5178, Val Loss = 0.5581, F1 = 0.9106
  Epoch 5: Train Loss = 0.5142, Val Loss = 0.5581, F1 = 0.9106
  Epoch 6: Train Loss = 0.5168, Val Loss = 0.5581, F1 = 0.9106
  Epoch 7: Train Loss = 0.5163, Val Loss = 0.5581, F1 = 0.9106
  Epoch 8: Train Loss = 0.5178, Val Loss = 0.5581, F1 = 0.9106
  Epoch 9: Train Loss = 0.5168, Val Loss = 0.5581, F1 = 0.9106
  Epoch 10: Train Loss = 0.5173, Val Loss = 0.5581, F1 = 0.9106
  Epoch 11: Train Loss = 0.5163, Val Loss = 0.5581, F1 = 0.9106
  Epoch 12: Train Loss = 0.5142, Val Loss = 0.5581, F1 = 0.9106
Early stopping

 Trying: LR = 0.0001, Batch Size = 32
  Epoch 1: Train Loss = 0.5270, Val Loss = 0.5325, F1 = 0.9106
  Epoch 2: Train Loss = 0.5147, Val Loss = 0.5292, F1 = 0.9106
  Epoch 3: Train Loss = 0.5189, Val Loss = 0.5287, F1 = 0.9106
  Epoch 4: Train Loss = 0.5200, Val Loss = 0.5286, F1 = 0.9106
  Epoch 5: Train Loss = 0.5139, Val Loss = 0.5286, F1 = 0.9106
  Epoch 6: Train Loss = 0.5200, Val Loss = 0.5286, F1 = 0.9106
  Epoch 7: Train Loss = 0.5127, Val Loss = 0.5286, F1 = 0.9106
  Epoch 8: Train Loss = 0.5188, Val Loss = 0.5286, F1 = 0.9106
  Epoch 9: Train Loss = 0.5163, Val Loss = 0.5286, F1 = 0.9106
  Epoch 10: Train Loss = 0.5114, Val Loss = 0.5286, F1 = 0.9106
  Epoch 11: Train Loss = 0.5175, Val Loss = 0.5286, F1 = 0.9106
  Epoch 12: Train Loss = 0.5139, Val Loss = 0.5286, F1 = 0.9106
  Epoch 13: Train Loss = 0.5139, Val Loss = 0.5286, F1 = 0.9106
  Epoch 14: Train Loss = 0.5151, Val Loss = 0.5286, F1 = 0.9106
  Epoch 15: Train Loss = 0.5163, Val Loss = 0.5286, F1 = 0.9106
  Epoch 16: Train Loss = 0.5163, Val Loss = 0.5286, F1 = 0.9106
  Epoch 17: Train Loss = 0.5139, Val Loss = 0.5286, F1 = 0.9106
  Epoch 18: Train Loss = 0.5175, Val Loss = 0.5286, F1 = 0.9106
  Epoch 19: Train Loss = 0.5126, Val Loss = 0.5286, F1 = 0.9106
  Epoch 20: Train Loss = 0.5163, Val Loss = 0.5286, F1 = 0.9106
  Epoch 21: Train Loss = 0.5200, Val Loss = 0.5286, F1 = 0.9106
  Epoch 22: Train Loss = 0.5114, Val Loss = 0.5286, F1 = 0.9106

"""

def train_model(model,
               train_ds,
               val_ds,
               lr: float = 5e-4,
               batch_size: int = 32,
               n_epochs: int = 3,
               patience: int = 5):
    """
    Trains a CRNN model on audio segment data with early stopping and LR scheduling.

    Args:
        model (torch.nn.Module): The CRNN model to train.
        train_ds (Dataset): PyTorch Dataset for training data.
        val_ds (Dataset): PyTorch Dataset for validation data.
        lr (float): Initial learning rate.
        batch_size (int): Batch size.
        n_epochs (int): Maximum number of training epochs.
        patience (int): Early stopping patience.

    Returns:
        torch.nn.Module: Trained model with best validation loss.
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    criterion = nn.BCEWithLogitsLoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size)

    best_val_loss = float('inf')
    best_state_dict = None
    epochs_no_improve = 0

    for epoch in range(1, n_epochs + 1):
        model.train()
        running_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)

            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
            optimizer.step()

            running_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_val, y_val in val_loader:
                X_val, y_val = X_val.to(device), y_val.to(device).unsqueeze(1)
                outputs = model(X_val)
                val_loss += criterion(outputs, y_val).item()
        val_loss /= len(val_loader)
        scheduler.step(val_loss)

        print(f"Epoch {epoch}: Train Loss = {running_loss / len(train_loader):.4f}, Val Loss = {val_loss:.4f}")

        # Early Stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state_dict = model.state_dict()
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print(f"Early stopping triggered after {epoch} epochs.")
                break

    model.load_state_dict(best_state_dict)
    return model

def main():
    #Load your splits and features as before
    sample_test, sample_train, sample_val = get_samples_id_perSet('/content/dataset_BeeNoBee_60sec/split_random_0.json')
    path = '/content/dataset_BeeNoBee_60sec/'
    labels2read = 'labels_BeeNotBee_th0'
    feature = 'MFCCs20'

    # Get ground-truth labels
    Y_train = labels2binary('nobee', get_GT_labels_fromFiles(path, sample_train, labels2read))
    Y_val   = labels2binary('nobee', get_GT_labels_fromFiles(path, sample_val,   labels2read))
    Y_test  = labels2binary('nobee', get_GT_labels_fromFiles(path, sample_test,  labels2read))

    # Build datasets (choose max_len based on longest MFCC time axis in your data)
    max_len = 200
    train_ds = AudioSegmentDataset(sample_train, Y_train, path, feature, max_len)
    val_ds   = AudioSegmentDataset(sample_val,   Y_val,   path, feature, max_len)
    test_ds  = AudioSegmentDataset(sample_test,  Y_test,  path, feature, max_len)

    ## Instantiate and train CNN
    model = CNN(n_freqs=20)

    # Train with early stopping
    model = train_model(model, train_ds, val_ds,
                       lr=1e-3, batch_size=32, n_epochs=3, patience=5)

    # Evaluate on test set (reuse your metric code)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)
    all_preds = []
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(device)  # shape: (B, 1, n_mfcc, time)
            y = y.to(device)

            logits = model(x)
            probs = torch.sigmoid(logits).squeeze()
            preds = (probs > 0.5).long()

            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    # Convert to numpy arrays
    Y_pred = np.array(all_preds)
    Y_scores = np.array(all_probs)
    Y_true = np.array(all_labels)

    # --- Core Metrics ---
    accuracy = accuracy_score(Y_true, Y_pred)
    precision = precision_score(Y_true, Y_pred, zero_division=0)
    recall = recall_score(Y_true, Y_pred, zero_division=0)
    f1 = f1_score(Y_true, Y_pred, zero_division=0)
    auroc = roc_auc_score(Y_true, Y_scores)
    auprc = average_precision_score(Y_true, Y_scores)

    print("\nTest set evaluation:")
    print(f"  Accuracy : {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall   : {recall:.4f}")
    print(f"  F1-score : {f1:.4f}")
    print(f"  AU-ROC   : {auroc:.4f}")
    print(f"  AU-PRC   : {auprc:.4f}")

    # --- Confusion Matrix ---
    cm = confusion_matrix(Y_true, Y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["nobee", "bee"], yticklabels=["nobee", "bee"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix (Test Set)")
    plt.show()

    # --- ROC Curve ---
    fpr, tpr, _ = roc_curve(Y_true, Y_scores)
    plt.figure()
    plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auroc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve (Test Set)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # --- Precision-Recall Curve ---
    precision_vals, recall_vals, _ = precision_recall_curve(Y_true, Y_scores)
    plt.figure()
    plt.plot(recall_vals, precision_vals, label=f"PRC (AUC = {auprc:.2f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve (Test Set)")
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()

"""**Summary of CNN Evaluation:**

The CNN model, after training (with early stopping likely occurring around 3-8 epochs based on the validation loss/F1 convergence), achieved the following performance on the held-out test set:

*   **Accuracy (0.8093):** The model correctly classified approximately 80.9% of the test samples.
*   **Precision (0.7831):** When the model predicted the positive class ('nobee'), it was correct about 78.3% of the time.
*   **Recall (0.8549):** The model successfully identified approximately 85.5% of all actual positive instances ('nobee') in the test set. This high recall suggests the model is quite good at finding most of the positive cases.
*   **F1-score (0.8174):** This balanced metric indicates a strong performance, considering both precision and recall. A value of 0.8174 suggests a good trade-off between not missing positive cases and not incorrectly labeling negative cases as positive.
*   **AU-ROC (0.8944):** The Area Under the Receiver Operating Characteristic curve is high, indicating that the model can effectively distinguish between the positive and negative classes across different probability thresholds.
*   **AU-PRC (0.8826):** The Area Under the Precision-Recall curve is also high, which is particularly relevant for imbalanced datasets (though the class balance is not explicitly shown here). A high AU-PRC suggests the model maintains high precision as recall increases.

The training output log shows that the validation F1-score quickly reaches 0.9106 and stays there for several epochs before early stopping might occur. The test set metrics (Accuracy 0.8093, F1 0.8174) are slightly lower than the best observed validation F1, which is expected as validation performance often represents an upper bound. However, the test set metrics are still very strong, especially the high Recall and AU-ROC/AU-PRC scores.

In summary, the CNN model appears to be a robust classifier for this task, demonstrating good generalization capabilities on unseen data, particularly in identifying the positive class.

####Based on the provided evaluation metrics, the **CRNN model performs best**, showing superior Accuracy (0.8093), Recall (0.8549), F1-score (0.8174), AU-ROC (0.8944), and AU-PRC (0.8826) compared to SVM.
"""

